{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base path: /mnt/ssd-1/mechinterp/taras/Count_PIPNet\n"
     ]
    }
   ],
   "source": [
    "# Enable auto-reloading of imports when they have been modified\n",
    "from IPython import get_ipython\n",
    "ipython = get_ipython(); assert ipython is not None\n",
    "ipython.run_line_magic(\"load_ext\", \"autoreload\")\n",
    "ipython.run_line_magic(\"autoreload\", \"2\")\n",
    "\n",
    "import os\n",
    "import json\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import sys\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Disable gradient computation - this notebook will only perform forward passes\n",
    "torch.set_grad_enabled(False)\n",
    "\n",
    "from pathlib import Path\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Add the base (root) directory to the path so we can import the util modules\n",
    "def get_base_folder(project_root = \"Count_PIPNet\"):\n",
    "\t# Find the project root dynamically\n",
    "\tcurrent_dir = os.getcwd()\n",
    "\twhile True:\n",
    "\t\tif os.path.basename(current_dir) == project_root:  # Adjust to match your project root folder name\n",
    "\t\t\tbreak\n",
    "\t\tparent = os.path.dirname(current_dir)\n",
    "\t\tif parent == current_dir:  # Stop if we reach the system root (failsafe)\n",
    "\t\t\traise RuntimeError(f\"Project root {project_root} not found. Check your folder structure.\")\n",
    "\t\tcurrent_dir = parent\n",
    "\n",
    "\treturn Path(current_dir)\n",
    "\n",
    "base_path = get_base_folder()\n",
    "print(f\"Base path: {base_path}\")\n",
    "sys.path.append(str(base_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from util.vis_pipnet import visualize_topk\n",
    "from pipnet.count_pipnet import get_count_network\n",
    "from util.checkpoint_manager import CheckpointManager\n",
    "from util.data import get_dataloaders\n",
    "from util.args import get_args\n",
    "from util.vis_pipnet import visualize_topk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Using cuda:2 device <<<\n"
     ]
    }
   ],
   "source": [
    "# Device setup\n",
    "GPU_TO_USE = 2\n",
    "\n",
    "if torch.backends.mps.is_available():\n",
    "    device = \"mps\"\n",
    "else:\n",
    "    device = f\"cuda:{GPU_TO_USE}\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "print(f'>>> Using {device} device <<<')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load a model from a checkpoint\n",
    "def load_model(run_dir, checkpoint_name='net_trained_best', base_path=base_path, gpu_id=GPU_TO_USE):\n",
    "    \"\"\"\n",
    "    Load a model from a checkpoint directory for evaluation purposes.\n",
    "\n",
    "    Args:\n",
    "        run_dir: Directory containing the run results\n",
    "        checkpoint_name: Name of checkpoint to load (default: 'net_trained_best')\n",
    "        base_path: Base path for dataset directories\n",
    "        gpu_id: GPU ID to use\n",
    "        \n",
    "    Returns:\n",
    "        Tuple of (net, trainloader, testloader, classes, args, is_count_pipnet)\n",
    "    \"\"\"\n",
    "    # Step 1: Load the configuration used for this run\n",
    "    metadata_dir = os.path.join(run_dir, 'metadata')\n",
    "    args_path = os.path.join(metadata_dir, 'args.pickle')\n",
    "\n",
    "    import pickle\n",
    "    with open(args_path, 'rb') as f:\n",
    "        args = pickle.load(f)\n",
    "    print(f\"Loaded configuration from {args_path}\")\n",
    "\n",
    "    # Explicitly set GPU ID to ensure device consistency\n",
    "    if torch.cuda.is_available():\n",
    "        args.gpu_ids = str(gpu_id)\n",
    "        device = torch.device(f'cuda:{gpu_id}')\n",
    "        torch.cuda.set_device(device)\n",
    "    else:\n",
    "        device = torch.device('cpu')\n",
    "\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    # Step 2: Create dataloaders\n",
    "    args.log_dir = run_dir  # Use the run directory as log_dir\n",
    "    trainloader, trainloader_pretraining, trainloader_normal, \\\n",
    "    trainloader_normal_augment, projectloader, testloader, test_projectloader, classes = get_dataloaders(args, device, base_path)\n",
    "\n",
    "    # Step 3: Create a model with the same architecture\n",
    "    if hasattr(args, 'model') and args.model == 'count_pipnet':\n",
    "        is_count_pipnet = True\n",
    "        net, num_prototypes = get_count_network(\n",
    "            num_classes=len(classes), \n",
    "            args=args,\n",
    "            max_count=getattr(args, 'max_count', 3),\n",
    "            use_ste=getattr(args, 'use_ste', False))\n",
    "    else:\n",
    "        is_count_pipnet = False\n",
    "        net, num_prototypes = get_pipnet(len(classes), args)\n",
    "\n",
    "    # Step 4: Move model to device\n",
    "    net = net.to(device)\n",
    "\n",
    "    # Step 5: Forward one batch through the backbone to get the latent output size\n",
    "    with torch.no_grad():\n",
    "        # Use a small batch to determine output shape\n",
    "        xs1, _, _ = next(iter(trainloader))\n",
    "        xs1 = xs1.to(device)\n",
    "\n",
    "        # Single-forward pass without DataParallel\n",
    "        features = net._net(xs1)\n",
    "        proto_features = net._add_on(features)\n",
    "\n",
    "        wshape = proto_features.shape[-1]\n",
    "        args.wshape = wshape  # needed for calculating image patch size\n",
    "        print(f\"Output shape: {proto_features.shape}, setting wshape={wshape}\")\n",
    "            \n",
    "    # Step 6: Now wrap with DataParallel\n",
    "    device_ids = [gpu_id]\n",
    "    print(f\"Using device_ids: {device_ids}\")\n",
    "    net = nn.DataParallel(net, device_ids=device_ids)\n",
    "\n",
    "    # Step 7: Direct checkpoint loading\n",
    "    checkpoint_path = os.path.join(run_dir, 'checkpoints', checkpoint_name)\n",
    "    if not os.path.exists(checkpoint_path):\n",
    "        print(f\"Checkpoint not found at {checkpoint_path}, trying alternative paths...\")\n",
    "        # Try with full path as fallback\n",
    "        if os.path.exists(checkpoint_name):\n",
    "            checkpoint_path = checkpoint_name\n",
    "        else:\n",
    "            # Try other common checkpoint names\n",
    "            alternatives = [\n",
    "                os.path.join(run_dir, 'checkpoints', 'net_trained_last'),\n",
    "                os.path.join(run_dir, 'checkpoints', 'net_trained'),\n",
    "                checkpoint_name # in case the direct path was passed\n",
    "            ]\n",
    "            for alt_path in alternatives:\n",
    "                if os.path.exists(alt_path):\n",
    "                    checkpoint_path = alt_path\n",
    "                    print(f\"Found alternative checkpoint at {checkpoint_path}\")\n",
    "                    break\n",
    "            else:\n",
    "                print(\"No checkpoint found\")\n",
    "                return None, None, None, None, None, None\n",
    "\n",
    "    try:\n",
    "        # Load just the model state dict\n",
    "        checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "        \n",
    "        if 'model_state_dict' in checkpoint:\n",
    "            net.load_state_dict(checkpoint['model_state_dict'], strict=True)\n",
    "            print(f\"Successfully loaded model state from {checkpoint_path}\")\n",
    "            \n",
    "            # Display additional information if available\n",
    "            if 'epoch' in checkpoint:\n",
    "                print(f\"Checkpoint from epoch {checkpoint['epoch']}\")\n",
    "            if 'accuracy' in checkpoint:\n",
    "                print(f\"Model accuracy: {checkpoint['accuracy']:.4f}\")\n",
    "            \n",
    "            return net, trainloader, testloader, classes, args, is_count_pipnet\n",
    "        else:\n",
    "            print(f\"Checkpoint doesn't contain model_state_dict\")\n",
    "            return None, None, None, None, None, None\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error loading checkpoint: {str(e)}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None, None, None, None, None, None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "# Suppress SettingWithCopyWarning\n",
    "warnings.simplefilter(action='ignore', category=pd.errors.SettingWithCopyWarning)\n",
    "# Suppress FutureWarning\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to evaluate a model on the test set\n",
    "def evaluate_model(net, testloader, device, args):\n",
    "    \"\"\"\n",
    "    Evaluate a model on the test set.\n",
    "    \n",
    "    Args:\n",
    "        net: The model to evaluate\n",
    "        testloader: DataLoader for the test set\n",
    "        device: Device to run evaluation on\n",
    "        args: Arguments object containing parameters\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary containing evaluation metrics\n",
    "    \"\"\"\n",
    "    print(\"Evaluating model on test set...\")\n",
    "    net.eval()\n",
    "    \n",
    "    # Set enforce_weight_sparsity parameter based on args if available\n",
    "    enforce_weight_sparsity = getattr(args, 'enforce_weight_sparsity', True)\n",
    "    \n",
    "    # Call the evaluation function from pipnet.test\n",
    "    eval_info = eval_pipnet(net, testloader, \"evaluation\", device, log=None, \n",
    "                           enforce_weight_sparsity=enforce_weight_sparsity, args=args)\n",
    "    \n",
    "    print(f\"Evaluation completed. Top-1 accuracy: {eval_info['top1_accuracy']:.4f}\")\n",
    "    \n",
    "    return eval_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load and parse the log_epoch_overview.csv file\n",
    "def load_training_metrics(run_dir):\n",
    "    \"\"\"\n",
    "    Load the training metrics from the log_epoch_overview.csv file.\n",
    "    \n",
    "    Args:\n",
    "        run_dir: Directory containing the run results\n",
    "        \n",
    "    Returns:\n",
    "        pandas DataFrame containing the metrics\n",
    "    \"\"\"\n",
    "    log_path = os.path.join(run_dir, 'log_epoch_overview.csv')\n",
    "    \n",
    "    if not os.path.exists(log_path):\n",
    "        print(f\"Log file not found at {log_path}\")\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        # Load the CSV file\n",
    "        df = pd.read_csv(log_path)\n",
    "        \n",
    "        # Convert 'n.a.' strings to NaN\n",
    "        df = df.replace('n.a.', np.nan)\n",
    "        \n",
    "        # Convert to numeric types where possible\n",
    "        for col in df.columns:\n",
    "            df[col] = pd.to_numeric(df[col], errors='ignore')\n",
    "        \n",
    "        # Determine which epochs were pretraining\n",
    "        pretraining_mask = df['test_top1_acc'].isna()\n",
    "        df['phase'] = 'training'\n",
    "        df.loc[pretraining_mask, 'phase'] = 'pretraining'\n",
    "        \n",
    "        # Calculate the real epoch number (pretraining + training)\n",
    "        df['real_epoch'] = df.index + 1\n",
    "        \n",
    "        # Calculate the training epoch number (just for the training phase)\n",
    "        training_epochs = (~pretraining_mask).cumsum()\n",
    "        df.loc[~pretraining_mask, 'training_epoch'] = training_epochs[~pretraining_mask]\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error loading log file: {str(e)}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to plot training metrics\n",
    "def plot_training_metrics(df, output_dir, run_name=None):\n",
    "    \"\"\"\n",
    "    Plot training metrics from the log_epoch_overview.csv file.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame containing the metrics\n",
    "        output_dir: Directory to save the plots\n",
    "        run_name: Name of the run (for labeling plots)\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary of plotly figure objects\n",
    "    \"\"\"\n",
    "    if df is None:\n",
    "        print(\"No metrics data to plot\")\n",
    "        return {}\n",
    "    \n",
    "    # Create directory for plots\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Create figures dictionary\n",
    "    figures = {}\n",
    "    \n",
    "    # Helper function to create and save a plot\n",
    "    def create_metric_plot(y_col, title, y_axis_title, pretraining=False, use_training_epoch=False):\n",
    "        if pretraining:\n",
    "            data = df[df['phase'] == 'pretraining']\n",
    "            x_col = 'real_epoch'\n",
    "        else:\n",
    "            data = df[df['phase'] == 'training']\n",
    "            x_col = 'training_epoch' if use_training_epoch else 'real_epoch'\n",
    "        \n",
    "        if data[y_col].isna().all():\n",
    "            print(f\"No data for {y_col} in {'pretraining' if pretraining else 'training'} phase\")\n",
    "            return None\n",
    "        \n",
    "        fig = px.line(data, x=x_col, y=y_col, \n",
    "                     title=f\"{title}{' - ' + run_name if run_name else ''}\",\n",
    "                     labels={'x': 'Epoch', 'y': y_axis_title})\n",
    "        \n",
    "        # Add horizontal line for best performance if applicable\n",
    "        if y_col == 'test_top1_acc' and not pretraining:\n",
    "            best_acc = data[y_col].max()\n",
    "            best_epoch = data[data[y_col] == best_acc][x_col].values[0]\n",
    "            \n",
    "            fig.add_shape(\n",
    "                type=\"line\",\n",
    "                x0=data[x_col].min(),\n",
    "                x1=data[x_col].max(),\n",
    "                y0=best_acc,\n",
    "                y1=best_acc,\n",
    "                line=dict(color=\"red\", dash=\"dash\"),\n",
    "            )\n",
    "            \n",
    "            fig.add_annotation(\n",
    "                x=best_epoch,\n",
    "                y=best_acc,\n",
    "                text=f\"Best: {best_acc:.4f} at epoch {best_epoch}\",\n",
    "                showarrow=True,\n",
    "                arrowhead=1,\n",
    "            )\n",
    "        \n",
    "        # Save the plot\n",
    "        if pretraining:\n",
    "            fig_path = os.path.join(output_dir, f\"pretraining_{y_col}.html\")\n",
    "        else:\n",
    "            fig_path = os.path.join(output_dir, f\"training_{y_col}.html\")\n",
    "        \n",
    "        fig.write_html(fig_path)\n",
    "        \n",
    "        return fig\n",
    "    \n",
    "    # Plot test accuracy \n",
    "    figures['test_accuracy'] = create_metric_plot(\n",
    "        'test_top1_acc', 'Test Accuracy', 'Accuracy', pretraining=False, use_training_epoch=True\n",
    "    )\n",
    "    \n",
    "    # Plot number of non-zero prototypes\n",
    "    figures['num_nonzero_prototypes'] = create_metric_plot(\n",
    "        'num_nonzero_prototypes', 'Number of Non-Zero Prototypes', 'Count', pretraining=False, use_training_epoch=True\n",
    "    )\n",
    "    \n",
    "    # Plot local size for all classes\n",
    "    figures['local_size_all_classes'] = create_metric_plot(\n",
    "        'local_size_all_classes', 'Local Size for All Classes', 'Size', pretraining=False, use_training_epoch=True\n",
    "    )\n",
    "    \n",
    "    # Plot training loss components for pretraining\n",
    "    pretraining_loss_cols = ['align_loss_raw', 'tanh_loss_raw']\n",
    "    if any(df[df['phase'] == 'pretraining'][col].notna().any() for col in pretraining_loss_cols):\n",
    "        pre_data = df[df['phase'] == 'pretraining']\n",
    "        pre_data_melt = pd.melt(pre_data.reset_index(), \n",
    "                               id_vars=['index', 'real_epoch'], \n",
    "                               value_vars=pretraining_loss_cols,\n",
    "                               var_name='Loss Component',\n",
    "                               value_name='Value')\n",
    "        \n",
    "        fig = px.line(pre_data_melt, x='real_epoch', y='Value', \n",
    "                     color='Loss Component',\n",
    "                     title=f\"Pretraining Loss Components{' - ' + run_name if run_name else ''}\",\n",
    "                     labels={'real_epoch': 'Epoch', 'Value': 'Loss Value'})\n",
    "        \n",
    "        # Save the plot\n",
    "        fig_path = os.path.join(output_dir, \"pretraining_loss_components.html\")\n",
    "        fig.write_html(fig_path)\n",
    "        figures['pretraining_loss_components'] = fig\n",
    "    \n",
    "    # Plot training loss components for training\n",
    "    training_loss_cols = ['align_loss_raw', 'tanh_loss_raw', 'class_loss_raw']\n",
    "    if any(df[df['phase'] == 'training'][col].notna().any() for col in training_loss_cols):\n",
    "        train_data = df[df['phase'] == 'training']\n",
    "        train_data_melt = pd.melt(train_data.reset_index(), \n",
    "                                 id_vars=['index', 'training_epoch'], \n",
    "                                 value_vars=training_loss_cols,\n",
    "                                 var_name='Loss Component',\n",
    "                                 value_name='Value')\n",
    "        \n",
    "        fig = px.line(train_data_melt, x='training_epoch', y='Value', \n",
    "                     color='Loss Component',\n",
    "                     title=f\"Training Loss Components{' - ' + run_name if run_name else ''}\",\n",
    "                     labels={'training_epoch': 'Epoch', 'Value': 'Loss Value'})\n",
    "        \n",
    "        # Save the plot\n",
    "        fig_path = os.path.join(output_dir, \"training_loss_components.html\")\n",
    "        fig.write_html(fig_path)\n",
    "        figures['training_loss_components'] = fig\n",
    "    \n",
    "    # Plot prototype presence metrics\n",
    "    prototype_metrics = ['almost_nonzeros_pooled', 'almost_sim_nonzeros']\n",
    "    if any(df[df['phase'] == 'training'][col].notna().any() for col in prototype_metrics):\n",
    "        train_data = df[df['phase'] == 'training']\n",
    "        metrics_melt = pd.melt(train_data.reset_index(), \n",
    "                              id_vars=['index', 'training_epoch'], \n",
    "                              value_vars=prototype_metrics,\n",
    "                              var_name='Metric',\n",
    "                              value_name='Value')\n",
    "        \n",
    "        fig = px.line(metrics_melt, x='training_epoch', y='Value', \n",
    "                     color='Metric',\n",
    "                     title=f\"Prototype Activation Metrics{' - ' + run_name if run_name else ''}\",\n",
    "                     labels={'training_epoch': 'Epoch', 'Value': 'Count'})\n",
    "        \n",
    "        # Save the plot\n",
    "        fig_path = os.path.join(output_dir, \"prototype_activation_metrics.html\")\n",
    "        fig.write_html(fig_path)\n",
    "        figures['prototype_activation'] = fig\n",
    "    \n",
    "    print(f\"Plots saved to {output_dir}\")\n",
    "    return figures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_models(run_dirs, output_dir, metric='test_top1_acc', display_in_notebook=True):\n",
    "    \"\"\"\n",
    "    Compare multiple models based on a specific metric from their log files.\n",
    "    \n",
    "    Args:\n",
    "        run_dirs: Dictionary mapping run names to run directories\n",
    "        output_dir: Directory to save the comparison plot\n",
    "        metric: Metric to compare (default: 'test_top1_acc')\n",
    "        display_in_notebook: Whether to display the figure in the notebook\n",
    "        \n",
    "    Returns:\n",
    "        plotly figure object\n",
    "    \"\"\"\n",
    "    # Create a combined DataFrame\n",
    "    combined_data = []\n",
    "    \n",
    "    for run_name, run_dir in run_dirs.items():\n",
    "        df = load_training_metrics(run_dir)\n",
    "        if df is not None:\n",
    "            # Only use training phase data\n",
    "            training_df = df[df['phase'] == 'training']\n",
    "            if not training_df.empty and metric in training_df.columns:\n",
    "                # Add run name for identification\n",
    "                training_df['run'] = run_name\n",
    "                combined_data.append(training_df[['training_epoch', metric, 'run']])\n",
    "    \n",
    "    if not combined_data:\n",
    "        print(\"No data to compare\")\n",
    "        return None\n",
    "    \n",
    "    # Concatenate the DataFrames\n",
    "    combined_df = pd.concat(combined_data, ignore_index=True)\n",
    "    \n",
    "    # Create comparison plot\n",
    "    fig = px.line(combined_df, x='training_epoch', y=metric, \n",
    "                 color='run',\n",
    "                 title=f\"Comparison of {metric} Across Models\",\n",
    "                 labels={'training_epoch': 'Epoch', metric: metric})\n",
    "    \n",
    "    # Improve readability with grid and legend\n",
    "    fig.update_layout(\n",
    "        legend=dict(orientation=\"h\", yanchor=\"bottom\", y=1.02, xanchor=\"right\", x=1),\n",
    "        xaxis=dict(showgrid=True),\n",
    "        yaxis=dict(showgrid=True)\n",
    "    )\n",
    "    \n",
    "    # Save the plot\n",
    "    fig_path = os.path.join(output_dir, f\"comparison_{metric}.html\")\n",
    "    fig.write_html(fig_path)\n",
    "    \n",
    "    print(f\"Comparison plot saved to {fig_path}\")\n",
    "    \n",
    "    # Display the figure in the notebook if requested\n",
    "    if display_in_notebook:\n",
    "        from IPython.display import display\n",
    "        display(fig)\n",
    "    \n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to analyze a single run\n",
    "def analyze_run(run_dir, output_dir, evaluate_test=True, checkpoint_name='net_trained_best'):\n",
    "    \"\"\"\n",
    "    Analyze a single run by loading its model, evaluating on test set, and plotting metrics.\n",
    "    \n",
    "    Args:\n",
    "        run_dir: Directory containing the run results\n",
    "        output_dir: Directory to save analysis results\n",
    "        evaluate_test: Whether to evaluate the model on the test set\n",
    "        checkpoint_name: Name of checkpoint to load\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary containing analysis results\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Analyzing run: {os.path.basename(run_dir)}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    run_name = os.path.basename(run_dir)\n",
    "    run_output_dir = os.path.join(output_dir, run_name)\n",
    "    os.makedirs(run_output_dir, exist_ok=True)\n",
    "    \n",
    "    results = {'name': run_name, 'run_dir': run_dir}\n",
    "    \n",
    "    # Load training metrics\n",
    "    df = load_training_metrics(run_dir)\n",
    "    if df is not None:\n",
    "        results['metrics_df'] = df\n",
    "        \n",
    "        # Plot training metrics\n",
    "        figures = plot_training_metrics(df, run_output_dir, run_name)\n",
    "        results['figures'] = figures\n",
    "        \n",
    "        # Extract best performance\n",
    "        if 'test_top1_acc' in df.columns and not df['test_top1_acc'].isna().all():\n",
    "            best_acc = df['test_top1_acc'].max()\n",
    "            best_epoch = df[df['test_top1_acc'] == best_acc]['real_epoch'].values[0]\n",
    "            results['best_accuracy'] = best_acc\n",
    "            results['best_epoch'] = best_epoch\n",
    "            print(f\"Best accuracy from log: {best_acc:.4f} at epoch {best_epoch}\")\n",
    "    \n",
    "    # Evaluate on test set if requested\n",
    "    if evaluate_test:\n",
    "        print(\"Loading model for test evaluation...\")\n",
    "        net, trainloader, testloader, classes, args, is_count_pipnet = load_model(\n",
    "            run_dir, checkpoint_name=checkpoint_name\n",
    "        )\n",
    "        \n",
    "        if net is not None:\n",
    "            # Evaluate on test set\n",
    "            eval_info = evaluate_model(net, testloader, device, args)\n",
    "            results['eval_info'] = eval_info\n",
    "            \n",
    "            # Save evaluation results\n",
    "            eval_results_path = os.path.join(run_output_dir, 'test_evaluation.json')\n",
    "            with open(eval_results_path, 'w') as f:\n",
    "                # Convert any non-serializable objects to strings\n",
    "                serializable_info = {}\n",
    "                for k, v in eval_info.items():\n",
    "                    if isinstance(v, (int, float, str, bool, list, dict)) or v is None:\n",
    "                        serializable_info[k] = v\n",
    "                    else:\n",
    "                        serializable_info[k] = str(v)\n",
    "                        \n",
    "                json.dump(serializable_info, f, indent=2)\n",
    "                \n",
    "            print(f\"Test evaluation results saved to {eval_results_path}\")\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Seed group evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract meaningful name and seed from run directory name\n",
    "def extract_name_and_seed(run_name):\n",
    "    \"\"\"\n",
    "    Extract the meaningful name and seed from a run directory name.\n",
    "    \n",
    "    Args:\n",
    "        run_name: Name of the run directory\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of (meaningful_name, seed, full_name)\n",
    "    \"\"\"\n",
    "    import re\n",
    "    \n",
    "    # Try to match patterns like \"_s{seed}_\" or \"seed{seed}_\" in the name\n",
    "    seed_pattern = r'_s(\\d+)_|seed(\\d+)_'\n",
    "    seed_match = re.search(seed_pattern, run_name)\n",
    "    \n",
    "    if seed_match:\n",
    "        # Extract the seed value from the first successful group\n",
    "        seed = seed_match.group(1) if seed_match.group(1) else seed_match.group(2)\n",
    "        \n",
    "        # Remove date pattern (assuming format like 'YYYYMMDD_HHMMSS_')\n",
    "        date_pattern = r'^\\d{8}_\\d{6}_\\d+_'\n",
    "        name_without_date = re.sub(date_pattern, '', run_name)\n",
    "        \n",
    "        # Replace the seed part with a placeholder to get the meaningful name\n",
    "        meaningful_name = re.sub(seed_pattern, '_SEED_', name_without_date)\n",
    "        \n",
    "        return meaningful_name, seed, run_name\n",
    "    else:\n",
    "        # If no seed found, use the full name\n",
    "        return run_name, None, run_name\n",
    "\n",
    "# Function to group run directories by meaningful name (ignoring seed and date)\n",
    "def group_runs_by_config(run_dirs):\n",
    "    \"\"\"\n",
    "    Group run directories by configuration (ignoring seed and date).\n",
    "    \n",
    "    Args:\n",
    "        run_dirs: Dictionary mapping run names to run directories\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary mapping meaningful names to lists of (seed, run_dir) tuples\n",
    "    \"\"\"\n",
    "    config_groups = {}\n",
    "    \n",
    "    for run_name, run_dir in run_dirs.items():\n",
    "        meaningful_name, seed, full_name = extract_name_and_seed(run_name)\n",
    "        \n",
    "        if seed is not None:  # Only consider runs with identified seeds\n",
    "            if meaningful_name not in config_groups:\n",
    "                config_groups[meaningful_name] = []\n",
    "            \n",
    "            config_groups[meaningful_name].append((seed, run_dir, full_name))\n",
    "    \n",
    "    # Sort runs within each group by seed\n",
    "    for group_name in config_groups:\n",
    "        config_groups[group_name].sort(key=lambda x: int(x[0]))\n",
    "    \n",
    "    # Print summary of found groups\n",
    "    print(f\"Found {len(config_groups)} configuration groups:\")\n",
    "    for group_name, runs in config_groups.items():\n",
    "        seed_list = [seed for seed, _, _ in runs]\n",
    "        print(f\"  - {group_name}: {len(runs)} runs with seeds {', '.join(seed_list)}\")\n",
    "    \n",
    "    return config_groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to evaluate a group of runs with the same configuration but different seeds\n",
    "def evaluate_run_group(config_name, seed_runs, output_dir):\n",
    "    \"\"\"\n",
    "    Evaluate a group of runs with the same configuration but different seeds.\n",
    "    \n",
    "    Args:\n",
    "        config_name: Name of the configuration group\n",
    "        seed_runs: List of (seed, run_dir, full_name) tuples for runs in this group\n",
    "        output_dir: Directory to save evaluation results\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary containing evaluation results\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Evaluating configuration group: {config_name}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    group_output_dir = os.path.join(output_dir, config_name)\n",
    "    os.makedirs(group_output_dir, exist_ok=True)\n",
    "    \n",
    "    results = {\n",
    "        'config_name': config_name,\n",
    "        'seeds': [],\n",
    "        'test_accuracies': [],\n",
    "        'seed_metrics': {}\n",
    "    }\n",
    "    \n",
    "    # Initialize DataFrames for collecting metrics across seeds\n",
    "    metrics_dfs = []\n",
    "    \n",
    "    # Evaluate each seed\n",
    "    for seed, run_dir, full_name in seed_runs:\n",
    "        print(f\"\\nEvaluating seed {seed} from {os.path.basename(run_dir)}\")\n",
    "        \n",
    "        seed_output_dir = os.path.join(group_output_dir, f\"seed_{seed}\")\n",
    "        os.makedirs(seed_output_dir, exist_ok=True)\n",
    "        \n",
    "        # Load the model for this seed\n",
    "        net, trainloader, testloader, classes, args, is_count_pipnet = load_model(\n",
    "            run_dir, checkpoint_name='net_trained_best'\n",
    "        )\n",
    "        \n",
    "        if net is not None:\n",
    "            # Evaluate on test set\n",
    "            eval_info = evaluate_model(net, testloader, device, args)\n",
    "            \n",
    "            # Record results\n",
    "            results['seeds'].append(seed)\n",
    "            results['test_accuracies'].append(eval_info['top1_accuracy'])\n",
    "            results['seed_metrics'][seed] = eval_info\n",
    "            \n",
    "            # Save evaluation results for this seed\n",
    "            eval_results_path = os.path.join(seed_output_dir, 'test_evaluation.json')\n",
    "            with open(eval_results_path, 'w') as f:\n",
    "                # Convert any non-serializable objects to strings\n",
    "                serializable_info = {}\n",
    "                for k, v in eval_info.items():\n",
    "                    if isinstance(v, (int, float, str, bool, list, dict)) or v is None:\n",
    "                        serializable_info[k] = v\n",
    "                    else:\n",
    "                        serializable_info[k] = str(v)\n",
    "                        \n",
    "                json.dump(serializable_info, f, indent=2)\n",
    "        \n",
    "        # Load training metrics for this seed\n",
    "        df = load_training_metrics(run_dir)\n",
    "        if df is not None:\n",
    "            # Add seed information to the DataFrame\n",
    "            df['seed'] = seed\n",
    "            df['full_name'] = full_name\n",
    "            metrics_dfs.append(df)\n",
    "    \n",
    "    # Calculate mean and standard deviation of test accuracies\n",
    "    if results['test_accuracies']:\n",
    "        results['mean_accuracy'] = np.mean(results['test_accuracies'])\n",
    "        results['std_accuracy'] = np.std(results['test_accuracies'])\n",
    "        \n",
    "        print(f\"\\nTest accuracy across {len(results['seeds'])} seeds:\")\n",
    "        print(f\"  Mean: {results['mean_accuracy']:.4f}\")\n",
    "        print(f\"  Std: {results['std_accuracy']:.4f}\")\n",
    "        print(f\"  Range: {min(results['test_accuracies']):.4f} - {max(results['test_accuracies']):.4f}\")\n",
    "        \n",
    "        # Save summary of test accuracies\n",
    "        summary_path = os.path.join(group_output_dir, 'test_accuracy_summary.json')\n",
    "        with open(summary_path, 'w') as f:\n",
    "            json.dump({\n",
    "                'config_name': config_name,\n",
    "                'seeds': results['seeds'],\n",
    "                'test_accuracies': results['test_accuracies'],\n",
    "                'mean_accuracy': results['mean_accuracy'],\n",
    "                'std_accuracy': results['std_accuracy']\n",
    "            }, f, indent=2)\n",
    "    \n",
    "    # Merge metrics DataFrames and create plots with error bars\n",
    "    if metrics_dfs:\n",
    "        combined_df = pd.concat(metrics_dfs, ignore_index=True)\n",
    "        results['combined_metrics'] = combined_df\n",
    "        \n",
    "        # Plot metrics with error bars\n",
    "        plot_group_metrics(combined_df, group_output_dir, config_name)\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to plot metrics for a group of runs with error bars\n",
    "def plot_group_metrics(combined_df, output_dir, config_name, metrics_to_plot=None):\n",
    "    \"\"\"\n",
    "    Plot metrics for a group of runs with the same configuration but different seeds,\n",
    "    showing mean and standard deviation as error bars.\n",
    "    \n",
    "    Args:\n",
    "        combined_df: DataFrame containing metrics for all seeds\n",
    "        output_dir: Directory to save plots\n",
    "        config_name: Name of the configuration group\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary of plotly figure objects\n",
    "    \"\"\"\n",
    "    figures = {}\n",
    "\n",
    "    if metrics_to_plot is None:\n",
    "        # List of metrics to plot\n",
    "        metrics_to_plot = [\n",
    "            ('test_top1_acc', 'Test Accuracy', 'Accuracy'),\n",
    "            ('num_nonzero_prototypes', 'Number of Non-Zero Prototypes', 'Count'),\n",
    "            ('local_size_all_classes', 'Local Size for All Classes', 'Size'),\n",
    "            ('almost_nonzeros_pooled', 'Almost Non-Zeros Pooled', 'Count'),\n",
    "            ('almost_sim_nonzeros', 'Almost Similarity Non-Zeros', 'Count')\n",
    "        ]\n",
    "    \n",
    "    # Only keep training phase data\n",
    "    training_df = combined_df[combined_df['phase'] == 'training']\n",
    "    \n",
    "    # For each metric\n",
    "    for metric, title, y_axis_title in metrics_to_plot:\n",
    "        if metric in training_df.columns and not training_df[metric].isna().all():\n",
    "            # Group by training epoch and calculate statistics\n",
    "            metrics_by_epoch = training_df.groupby('training_epoch')[metric].agg(['mean', 'std']).reset_index()\n",
    "            \n",
    "            # Create figure with error bars\n",
    "            fig = go.Figure()\n",
    "            \n",
    "            # Add mean line\n",
    "            fig.add_trace(go.Scatter(\n",
    "                x=metrics_by_epoch['training_epoch'],\n",
    "                y=metrics_by_epoch['mean'],\n",
    "                mode='lines+markers',\n",
    "                name='Mean',\n",
    "                line=dict(color='royalblue'),\n",
    "                marker=dict(size=8)\n",
    "            ))\n",
    "            \n",
    "            # Add error bands (mean ± std)\n",
    "            fig.add_trace(go.Scatter(\n",
    "                x=metrics_by_epoch['training_epoch'],\n",
    "                y=metrics_by_epoch['mean'] + metrics_by_epoch['std'],\n",
    "                mode='lines',\n",
    "                name='Upper Bound',\n",
    "                line=dict(width=0),\n",
    "                showlegend=False\n",
    "            ))\n",
    "            \n",
    "            fig.add_trace(go.Scatter(\n",
    "                x=metrics_by_epoch['training_epoch'],\n",
    "                y=metrics_by_epoch['mean'] - metrics_by_epoch['std'],\n",
    "                mode='lines',\n",
    "                name='Lower Bound',\n",
    "                line=dict(width=0),\n",
    "                fill='tonexty',\n",
    "                fillcolor='rgba(65, 105, 225, 0.2)',\n",
    "                showlegend=False\n",
    "            ))\n",
    "            \n",
    "            # Update layout\n",
    "            fig.update_layout(\n",
    "                title=f\"{title} (Mean ± Std across seeds) - {config_name}\",\n",
    "                xaxis_title=\"Epoch\",\n",
    "                yaxis_title=y_axis_title,\n",
    "                legend=dict(orientation=\"h\", yanchor=\"bottom\", y=1.02, xanchor=\"right\", x=1),\n",
    "                xaxis=dict(showgrid=True),\n",
    "                yaxis=dict(showgrid=True),\n",
    "                hovermode=\"x unified\"\n",
    "            )\n",
    "            \n",
    "            # Save figure\n",
    "            fig_path = os.path.join(output_dir, f\"{metric}_with_error_bars.html\")\n",
    "            fig.write_html(fig_path)\n",
    "            \n",
    "            # Display the figure in the notebook\n",
    "            from IPython.display import display\n",
    "            display(fig)\n",
    "            \n",
    "            figures[metric] = fig\n",
    "    \n",
    "    # Also create a plot showing all individual seed trajectories\n",
    "    for metric, title, y_axis_title in metrics_to_plot:\n",
    "        if metric in training_df.columns and not training_df[metric].isna().all():\n",
    "            # Create figure for individual seeds\n",
    "            fig = px.line(\n",
    "                training_df, \n",
    "                x='training_epoch', \n",
    "                y=metric, \n",
    "                color='seed',\n",
    "                title=f\"{title} (Individual Seeds) - {config_name}\",\n",
    "                labels={'training_epoch': 'Epoch', metric: y_axis_title}\n",
    "            )\n",
    "            \n",
    "            # Update layout\n",
    "            fig.update_layout(\n",
    "                legend=dict(orientation=\"h\", yanchor=\"bottom\", y=1.02, xanchor=\"right\", x=1),\n",
    "                xaxis=dict(showgrid=True),\n",
    "                yaxis=dict(showgrid=True)\n",
    "            )\n",
    "            \n",
    "            # Save figure\n",
    "            fig_path = os.path.join(output_dir, f\"{metric}_individual_seeds.html\")\n",
    "            fig.write_html(fig_path)\n",
    "            \n",
    "            # Display the figure in the notebook\n",
    "            from IPython.display import display\n",
    "            display(fig)\n",
    "            \n",
    "            figures[f\"{metric}_individual\"] = fig\n",
    "    \n",
    "    return figures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment directory: /mnt/ssd-1/mechinterp/taras/Count_PIPNet/runs/multi_stage\n"
     ]
    }
   ],
   "source": [
    "# Define the base directory for your experiments\n",
    "multi_experiment_dir = base_path / 'runs/multi_stage' \n",
    "print(f\"Experiment directory: {multi_experiment_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation results will be saved to /mnt/ssd-1/mechinterp/taras/Count_PIPNet/evaluations\n"
     ]
    }
   ],
   "source": [
    "# Create a directory for saving evaluation results\n",
    "eval_results_dir = base_path / 'evaluations'\n",
    "os.makedirs(eval_results_dir, exist_ok=True)\n",
    "print(f\"Evaluation results will be saved to {eval_results_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary file NOT found at /mnt/ssd-1/mechinterp/taras/Count_PIPNet/runs/multi_stage/summary.json. Please ensure the training was completed and the summary file was generated.\n"
     ]
    }
   ],
   "source": [
    "summary_path = os.path.join(multi_experiment_dir, 'summary.json')\n",
    "\n",
    "try:\n",
    "\t# Load the summary file to get all run directories\n",
    "\twith open(summary_path, 'r') as f:\n",
    "\t\tsummary = json.load(f)\n",
    "\n",
    "\tprint(f\"Found {len(summary)} trained models\")\n",
    "except FileNotFoundError:\n",
    "    summary = None\n",
    "    print(f\"Summary file NOT found at {summary_path}. Please ensure the training was completed and the summary file was generated.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 2 manually specified run directories\n"
     ]
    }
   ],
   "source": [
    "# Option to use hard-coded run directories (specify just the base names)\n",
    "hard_coded_run_dirs = [\n",
    "    '45_shapesGN_linear',\n",
    "    '45_shapesGN_onehot'\n",
    "]\n",
    "\n",
    "if hard_coded_run_dirs:\n",
    "    # Convert base names to full paths (assume they're in the 'runs/' folder)\n",
    "    run_dirs = {}\n",
    "    for run_name in hard_coded_run_dirs:\n",
    "        full_path = base_path / 'runs' / run_name\n",
    "        if os.path.isdir(full_path) and os.path.exists(os.path.join(full_path, 'metadata')):\n",
    "            run_dirs[run_name] = str(full_path)\n",
    "        else:\n",
    "            print(f\"Warning: Run directory '{run_name}' not found at {full_path}\")\n",
    "    print(f\"Using {len(run_dirs)} manually specified run directories\")\n",
    "elif summary is None:\n",
    "    run_dirs = {}\n",
    "    for item in os.listdir(multi_experiment_dir):\n",
    "        item_path = os.path.join(multi_experiment_dir, item)\n",
    "        if os.path.isdir(item_path) and os.path.exists(os.path.join(item_path, 'metadata')):\n",
    "            run_dirs[item] = item_path\n",
    "    print(f\"Found {len(run_dirs)} run directories\")\n",
    "else:\n",
    "    # Use the runs from the summary file\n",
    "    run_dirs = {os.path.basename(run['output_dir']): run['output_dir'] for run in summary}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'45_shapesGN_linear': '/mnt/ssd-1/mechinterp/taras/Count_PIPNet/runs/45_shapesGN_linear',\n",
       " '45_shapesGN_onehot': '/mnt/ssd-1/mechinterp/taras/Count_PIPNet/runs/45_shapesGN_onehot'}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_dirs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main part (multiple seeds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 2 manually specified run directories\n"
     ]
    }
   ],
   "source": [
    "# Prepare run directories as before\n",
    "if hard_coded_run_dirs is not None:\n",
    "    # Convert base names to full paths (assume they're in the 'runs/' folder)\n",
    "    run_dirs = {}\n",
    "    for run_name in hard_coded_run_dirs:\n",
    "        full_path = base_path / 'runs' / run_name\n",
    "        if os.path.isdir(full_path) and os.path.exists(os.path.join(full_path, 'metadata')):\n",
    "            run_dirs[run_name] = str(full_path)\n",
    "        else:\n",
    "            print(f\"Warning: Run directory '{run_name}' not found at {full_path}\")\n",
    "    print(f\"Using {len(run_dirs)} manually specified run directories\")\n",
    "elif summary is None:\n",
    "    run_dirs = {}\n",
    "    for item in os.listdir(multi_experiment_dir):\n",
    "        item_path = os.path.join(multi_experiment_dir, item)\n",
    "        if os.path.isdir(item_path) and os.path.exists(os.path.join(item_path, 'metadata')):\n",
    "            run_dirs[item] = item_path\n",
    "    print(f\"Found {len(run_dirs)} run directories\")\n",
    "else:\n",
    "    # Use the runs from the summary file\n",
    "    run_dirs = {os.path.basename(run['output_dir']): run['output_dir'] for run in summary}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 0 configuration groups:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c07703ee85db4f3c82b4bf6b2eec4560",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating configuration groups: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Summary of results by configuration:\n",
      "\n",
      "Analysis complete!\n"
     ]
    }
   ],
   "source": [
    "# Group runs by configuration (ignoring seed and date)\n",
    "config_groups = group_runs_by_config(run_dirs)\n",
    "\n",
    "# Evaluate each configuration group\n",
    "all_group_results = []\n",
    "for config_name, seed_runs in tqdm(config_groups.items(), desc=\"Evaluating configuration groups\"):\n",
    "    # Only evaluate groups with multiple seeds\n",
    "    if len(seed_runs) >= 1:\n",
    "        group_result = evaluate_run_group(config_name, seed_runs, eval_results_dir)\n",
    "        all_group_results.append(group_result)\n",
    "\n",
    "# Create a summary table of results\n",
    "print(\"\\nSummary of results by configuration:\")\n",
    "summary_data = []\n",
    "for result in all_group_results:\n",
    "    if 'mean_accuracy' in result:\n",
    "        summary_data.append({\n",
    "            'config': result['config_name'],\n",
    "            'num_seeds': len(result['seeds']),\n",
    "            'mean_accuracy': result['mean_accuracy'],\n",
    "            'std_accuracy': result['std_accuracy'],\n",
    "            'min_accuracy': min(result['test_accuracies']),\n",
    "            'max_accuracy': max(result['test_accuracies'])\n",
    "        })\n",
    "\n",
    "if summary_data:\n",
    "    summary_df = pd.DataFrame(summary_data)\n",
    "    summary_df = summary_df.sort_values('mean_accuracy', ascending=False)\n",
    "    \n",
    "    # Save to CSV\n",
    "    summary_path = os.path.join(eval_results_dir, 'config_results_summary.csv')\n",
    "    summary_df.to_csv(summary_path, index=False)\n",
    "    \n",
    "    # Print summary\n",
    "    display(summary_df)\n",
    "    print(f\"\\nConfiguration summary saved to {summary_path}\")\n",
    "\n",
    "print(\"\\nAnalysis complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting validation metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of metrics to plot\n",
    "metrics_to_plot = [\n",
    "    ('test_top1_acc', 'Test Accuracy', 'Accuracy'),\n",
    "    ('num_nonzero_prototypes', 'Number of Non-Zero Prototypes', 'Count'),\n",
    "    ('local_size_all_classes', 'Local Size for All Classes', 'Size'),\n",
    "    ('almost_nonzeros_pooled', 'Almost Non-Zeros Pooled', 'Count'),\n",
    "    ('almost_sim_nonzeros', 'Almost Similarity Non-Zeros', 'Count')\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize metrics for each configuration group\n",
    "for config_name, seed_runs in config_groups.items():\n",
    "    print(f\"\\nPlotting metrics for configuration: {config_name}\")\n",
    "    \n",
    "    # Create output directory for this configuration\n",
    "    config_output_dir = os.path.join(eval_results_dir, config_name)\n",
    "    os.makedirs(config_output_dir, exist_ok=True)\n",
    "    \n",
    "    # Collect metrics DataFrames from all seeds in this configuration\n",
    "    metrics_dfs = []\n",
    "    for seed, run_dir, full_name in seed_runs:\n",
    "        df = load_training_metrics(run_dir)\n",
    "        if df is not None:\n",
    "            # Add seed information to the DataFrame\n",
    "            df['seed'] = seed\n",
    "            df['full_name'] = full_name\n",
    "            metrics_dfs.append(df)\n",
    "    \n",
    "    # Combine metrics from all seeds\n",
    "    if metrics_dfs:\n",
    "        combined_df = pd.concat(metrics_dfs, ignore_index=True)\n",
    "        \n",
    "        # Plot metrics with error bars\n",
    "        plot_group_metrics(combined_df, config_output_dir, config_name, metrics_to_plot=metrics_to_plot)\n",
    "    else:\n",
    "        print(f\"No metrics data available for configuration: {config_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
