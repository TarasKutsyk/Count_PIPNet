# CountPIPNet MNIST Configuration
model: count_pipnet               # Model type: 'pipnet' for original or 'count_pipnet' for counting-aware version
dataset: geometric_shapes_224_gaussian_noise           # Dataset name as defined in data.py

pretrained_checkpoints_dir: ''
resume_training: False

max_count: 3                      # Maximum count value to track; counts >= max_count get mapped to max_count category
use_ste: True                     # Whether to use Straight-Through Estimator for count discretization during training
use_mid_layers: True              # Use only middle layers of the backbone network
num_stages: 3                     # Number of stages to use when using middle layers
num_features: 16                   # Number of prototypes (0 = use backbone's output channel count)
activation: 'gumbel_softmax'  # Options: 'gumbel_softmax', 'softmax'
intermediate_layer: identity # Options: 'linear', 'onehot', 'bilinear', 'identity', 'linear_full'
enforce_weight_sparsity: True

# Logging and output
log_dir: ./runs/45_shapesGN_identity # Directory for saving logs and checkpoints
dir_for_saving_images: visualization_results_shapes # Directory for prototype visualizations

# Network architecture
net: convnext_tiny_26             # Backbone CNN architecture; 26 indicates output feature map size
image_size: 192                    # Input image resolution (height and width)
disable_pretrained: False         # When False, uses weights pretrained on ImageNet

# Training parameters
batch_size: 64                    # Batch size for main training phase
batch_size_pretrain: 128         # Batch size for prototype pretraining phase
epochs: 100                        # Number of training epochs (main phase)
epochs_pretrain: 10               # Number of prototype pretraining epochs
epochs_finetune: 0
freeze_epochs: 10                 # Number of epochs to keep early backbone layers frozen

# Optimization parameters
optimizer: Adam                   # Optimization algorithm for updating weights
lr: 0.005                         # Learning rate for classification layer
lr_block: 0.0005                  # Learning rate for middle backbone layers
lr_net: 0.0005                    # Learning rate for early backbone layers
weight_decay: 0.0                 # L2 regularization strength (0.0 = no regularization)

# Other parameters
validation_size: 0.0              # Fraction of training data to use for validation (0.0 = use separate test set)
weighted_loss: False              # Whether to weight loss by inverse class frequency
seed: 1                           # Random seed for reproducibility
gpu_ids: '3'                      # GPU IDs to use (comma-separated for multiple GPUs)
num_workers: 8                    # Number of CPU threads for data loading
bias: False                       # Whether to use bias terms in classification layer
disable_cuda: False               # When True, forces CPU usage even if GPU is available